{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANdiscriminator(nn.Module):\n",
    "    ''' This class implements a PatchGAN discriminator for a 100x100 image.\n",
    "        Small modification of the one used in:\n",
    "            - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks \n",
    "              Jun-Yan Zhu, 2017'''\n",
    "    \n",
    "    def __init__(self, n_image_channels = 3):\n",
    "                \n",
    "        def createLayer(n_filt_in, n_filt_out, ker_size, stride, norm = True, last = False):\n",
    "            ''' This function creates the differnt convolutional layers, all with same structure'''\n",
    "            layers = nn.Conv2d(n_filt_in, n_filt_out, ker_size, stride=stride)\n",
    "            if (norm):\n",
    "                layers.append(nn.InstanceNorm2d(n_filt_out)) # batch normalization\n",
    "            if (last):\n",
    "                layers.append(nn.Sigmoid()) # we output the probability\n",
    "            else:\n",
    "                layers.append(nn.LeakyReLU(negative_slope = 0.05, inplace=True)) # we use Leacky ReLU\n",
    "        \n",
    "        \n",
    "        ''' Input number of filters: Image channels\n",
    "            Intermediate number of filters: 64*h, with h being the depth of the layer\n",
    "            Output number of filters: 1 -> Decision of true or false\n",
    "            It takes patches of 61x61 pixels'''\n",
    "        n_layers = 5\n",
    "        ker_size = 5\n",
    "        strides = [1, 1, 1, 2, 2]\n",
    "        n_filters = [n_image_channels, 64, 128, 256, 512, 1]\n",
    "        lasts = [False, False, False, False, True]\n",
    "        for layer in range(n_layers): # For each layer\n",
    "            layers.extend(createLayer(n_filters[i], n_filters[i+1], ker_size, strides[i], lasts[i]))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual_block(nn.Module):\n",
    "    ''' This class implements the residual block of the RES net we will implement as the generator'''\n",
    "    def __init__(self, n_channels):\n",
    "        \n",
    "        layers = [ \n",
    "                  nn.ReflectionPad2d(1), # mirroring of 1 for the 3 kernel size convolution\n",
    "                  nn.Conv2d(n_channels, n_channels, 3), # the convolution :)\n",
    "                  nn.InstanceNorm2d(n_channels), # batch normalization\n",
    "                  nn.LeakyReLU(negative_slope=0.05, inplace=True), \n",
    "                  # We repeat the process\n",
    "                  nn.ReflectionPad2d(1), \n",
    "                  nn.Conv2d(n_channels, n_channels, 3),\n",
    "                  nn.InstanceNorm2d(n_channels)\n",
    "                 ]\n",
    "        \n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return image + self.conv_block(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANgenerator(nn.Module):\n",
    "    ''' This class implements a RES Net for generating the images\n",
    "        Small modification of the one defined in:\n",
    "            - Deep Residual Learning for Image Recognition\n",
    "              Kaiming He, 2015'''\n",
    "    \n",
    "    def __init__(self, n_image_channels = 3, n_res_blocks = 9):\n",
    "        \n",
    "        ''' High kernel convolution '''\n",
    "        n_channels_high = 64\n",
    "        layers = [ nn.ReflectionPad2d(3), # mirroring of 3 for the 7 kernel size convolution\n",
    "                    nn.Conv2d(n_image_channels, n_channels_high, 7), # 64 new channels of 7x7 convolution :)\n",
    "                    nn.InstanceNorm2d(n_channels_high),\n",
    "                    nn.LeakyReLU(negative_slope=0.05, inplace=True)\n",
    "                  ]\n",
    "        \n",
    "        ''' Variables for down and up sampling '''\n",
    "        n_layers = 2\n",
    "        ker_size = 3\n",
    "        strides = 2\n",
    "        paddings = 1\n",
    "        n_filters = [n_channels_high, n_channels_high*2, n_channels_high*4]\n",
    "        \n",
    "        ''' Downsampling steps '''\n",
    "        for i in range(n_layers): # for each layer\n",
    "            layers.extend([ nn.Conv2d(n_filters[i], n_filters[i+1], ker_size, \\\n",
    "                                    strides, padding=paddings),\n",
    "                            nn.InstanceNorm2d(n_filters[i+1]),\n",
    "                            nn.LeakyReLU(negative_slope=0.05, inplace=True)])\n",
    "        \n",
    "        ''' Residual blocks '''\n",
    "        for i in range(n_res_blocks):\n",
    "            layers.extend(residual_block(n_filters[-1])) # the residual blocks are applied to the \n",
    "                                                         #last number of channels in the down sampling\n",
    "        \n",
    "        ''' Upsampling steps '''\n",
    "        for i in range(n_layers): # for each layer\n",
    "            layers.extend([ nn.Conv2d(n_filters[-(i+1)], n_filters[-(i+2)], ker_size, \\\n",
    "                                    strides, padding=paddings),\n",
    "                            nn.InstanceNorm2d(n_filters[-(i+2)]),\n",
    "                            nn.LeakyReLU(negative_slope=0.05, inplace=True)])\n",
    "        ''' Output '''\n",
    "        layers.extend([ nn.ReflectionPad2d(3), # mirroring of 3 for the 7 kernel size convolution\n",
    "                        nn.Conv2d(n_channels_high, n_image_channels, 7), # 64 new channels of 7x7 convolution :)\n",
    "                        nn.Tanh() ])\n",
    "        \n",
    "        self.res_net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        return self.res_net(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "cuda = True if torch.cuda.is_available() else False # for using the GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
