{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import database as db\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANdiscriminator(nn.Module):\n",
    "    ''' This class implements a PatchGAN discriminator for a 100x100 image.\n",
    "        Small modification of the one used in:\n",
    "            - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks \n",
    "              Jun-Yan Zhu, 2017'''\n",
    "    \n",
    "    def __init__(self, n_image_channels = 3):\n",
    "        super(GANdiscriminator, self).__init__()\n",
    "                \n",
    "        def createLayer(n_filt_in, n_filt_out, ker_size, stride, norm = True, last = False):\n",
    "            ''' This function creates the differnt convolutional layers, all with same structure'''\n",
    "            layers = [nn.Conv2d(n_filt_in, n_filt_out, ker_size, stride=stride)]\n",
    "            if (norm):\n",
    "                layers.append(nn.InstanceNorm2d(n_filt_out)) # batch normalization\n",
    "            if (last):\n",
    "                layers.append(nn.Sigmoid()) # we output the probability\n",
    "            else:\n",
    "                layers.append(nn.LeakyReLU(negative_slope = 0.05, inplace=True)) # we use Leaky ReLU\n",
    "            return layers\n",
    "        \n",
    "        \n",
    "        ''' Input number of filters: Image channels\n",
    "            Intermediate number of filters: 64*h, with h being the depth of the layer\n",
    "            Output number of filters: 1 -> Decision of true or false\n",
    "            It takes patches of 61x61 pixels'''\n",
    "        layers = []\n",
    "        n_layers = 5\n",
    "        ker_size = 5\n",
    "        strides = [1, 1, 1, 2, 2]\n",
    "        n_filters = [n_image_channels, 64, 128, 256, 512, 1]\n",
    "        lasts = [False, False, False, False, True]\n",
    "        for i in range(n_layers): # For each layer\n",
    "            layers.extend(createLayer(n_filters[i], n_filters[i+1], ker_size, strides[i], last = lasts[i]))\n",
    "                \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual_block(nn.Module):\n",
    "    ''' This class implements the residual block of the RES net we will implement as the generator'''\n",
    "    def __init__(self, n_channels):\n",
    "        super(residual_block, self).__init__()\n",
    "        \n",
    "        layers = [ \n",
    "                  nn.ReflectionPad2d(1), # mirroring of 1 for the 3 kernel size convolution\n",
    "                  nn.Conv2d(n_channels, n_channels, 3), # the convolution :)\n",
    "                  nn.InstanceNorm2d(n_channels), # batch normalization\n",
    "                  nn.LeakyReLU(negative_slope=0.05, inplace=True), \n",
    "                  # We repeat the process\n",
    "                  nn.ReflectionPad2d(1), \n",
    "                  nn.Conv2d(n_channels, n_channels, 3),\n",
    "                  nn.InstanceNorm2d(n_channels)\n",
    "                 ]\n",
    "        \n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return image + self.conv_block(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANgenerator(nn.Module):\n",
    "    ''' This class implements a RES Net for generating the images\n",
    "        Small modification of the one defined in:\n",
    "            - Deep Residual Learning for Image Recognition\n",
    "              Kaiming He, 2015'''\n",
    "    \n",
    "    def __init__(self, n_image_channels = 3, n_res_blocks = 9):\n",
    "        super(GANgenerator, self).__init__()\n",
    "        \n",
    "        ''' High kernel convolution '''\n",
    "        n_channels_high = 64\n",
    "        layers = [ nn.ReflectionPad2d(3), # mirroring of 3 for the 7 kernel size convolution\n",
    "                    nn.Conv2d(n_image_channels, n_channels_high, 7), # 64 new channels of 7x7 convolution :)\n",
    "                    nn.InstanceNorm2d(n_channels_high),\n",
    "                    nn.LeakyReLU(negative_slope=0.05, inplace=True)\n",
    "                  ]\n",
    "        \n",
    "        ''' Variables for down and up sampling '''\n",
    "        n_layers = 2\n",
    "        ker_size = 3\n",
    "        strides = 2\n",
    "        paddings = 1\n",
    "        n_filters = [n_channels_high, n_channels_high*2, n_channels_high*4]\n",
    "        \n",
    "        ''' Downsampling steps '''\n",
    "        for i in range(n_layers): # for each layer\n",
    "            layers.extend([ nn.Conv2d(n_filters[i], n_filters[i+1], ker_size, \\\n",
    "                                    strides, padding=paddings),\n",
    "                            nn.InstanceNorm2d(n_filters[i+1]),\n",
    "                            nn.LeakyReLU(negative_slope=0.05, inplace=True)])\n",
    "        \n",
    "        ''' Residual blocks '''\n",
    "        for i in range(n_res_blocks):\n",
    "            layers.extend([residual_block(n_filters[-1])]) # the residual blocks are applied to the \n",
    "                                                           # last number of channels in the down sampling\n",
    "        \n",
    "        ''' Upsampling steps '''\n",
    "        for i in range(n_layers): # for each layer\n",
    "            layers.extend([ nn.ConvTranspose2d(n_filters[-(i+1)], n_filters[-(i+2)], ker_size, \\\n",
    "                                    strides, padding=paddings, output_padding=1),\n",
    "                            nn.InstanceNorm2d(n_filters[-(i+2)]),\n",
    "                            nn.LeakyReLU(negative_slope=0.05, inplace=True)])\n",
    "        ''' Output '''\n",
    "        layers.extend([ nn.ReflectionPad2d(3), # mirroring of 3 for the 7 kernel size convolution\n",
    "                        nn.Conv2d(n_channels_high, n_image_channels, 7), # 64 new channels of 7x7 convolution :)\n",
    "                        nn.Tanh() ])\n",
    "                \n",
    "        self.res_net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        return self.res_net(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "cuda = True if torch.cuda.is_available() else False # for using the GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of patches (61x61) separated 25\n",
    "batch_size = 4\n",
    "img_x = 128\n",
    "img_y = 128\n",
    "separation = 25\n",
    "patch_x, patch_y = 58, 58\n",
    "patch = (batch_size, 1 , patch_x, patch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of the discriminator and generator\n",
    "D = GANdiscriminator()\n",
    "G = GANgenerator()\n",
    "\n",
    "if cuda:\n",
    "    D = D.cuda()\n",
    "    G = G.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.kaiming_normal(m.weight.data, a=0.05)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.kaiming_normal(m.weight.data, a=0.05)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "G.apply(weights_init); # He initialization of the weights\n",
    "D.apply(weights_init); # He initialization of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer for the generator\n",
    "lr = 1e-4\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.99\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 58, 58)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scheduler (TO DO)\n",
    "patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "input_img = Tensor(batch_size,3,img_x,img_y)\n",
    "valid = Variable(Tensor(np.ones(patch)), requires_grad=False)\n",
    "fake = Variable(Tensor(np.zeros(patch)), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "fruits_file = 'Dataset/dataset_index.csv'\n",
    "textures_file = 'Dataset/textures_index.csv'\n",
    "textures = db.TexturesDataset(csv_file=textures_file)\n",
    "train_imgs = db.FruitsDataset(csv_file=fruits_file, cl='Strawberry', \n",
    "                        transform = transforms.Compose(\n",
    "                            [db.ChangeBackground(textures),\n",
    "                             db.myReshape()]))\n",
    "dataloader = DataLoader(train_imgs, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target and input must have the same number of elements. target nelement (13456) != input nelement (2704)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6799a9c41b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# See how well they do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# We use backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         return F.binary_cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 372\u001b[0;31m                                       size_average=self.size_average)\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m         raise ValueError(\"Target and input must have the same number of elements. target nelement ({}) \"\n\u001b[0;32m-> 1171\u001b[0;31m                          \"!= input nelement ({})\".format(target.nelement(), input.nelement()))\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target and input must have the same number of elements. target nelement (13456) != input nelement (2704)"
     ]
    }
   ],
   "source": [
    "# Training phase\n",
    "n_epochs = 1\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Get model input\n",
    "        input_img = Variable(batch.type(Tensor))\n",
    "        \n",
    "        ''' Generator '''\n",
    "        \n",
    "        # Set the optimizer\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Create some noise\n",
    "        noise = Variable(Tensor(np.random.normal(loc = 0.0, \\\n",
    "            scale= 0.5, size=batch.shape)))\n",
    "    \n",
    "        # Generate some images from the noise\n",
    "        generated = G(noise)\n",
    "        \n",
    "        print(generated.shape)\n",
    "        print(D(generated).shape)\n",
    "        print(valid.shape)\n",
    "        # See how well they do\n",
    "        loss_g = criterion(D(generated),valid)\n",
    "        \n",
    "        # We use backpropagation\n",
    "        loss_g.backward(retain_graph=True)\n",
    "        \n",
    "        # And make ADAM do its shit\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        ''' Discriminator '''\n",
    "        \n",
    "        # Set the optimizer\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        loss_d_real = criterion(D(input_img),valid)\n",
    "        loss_d_fake = criterion(D(generated),fake)\n",
    "        loss_d = 0.5 * (loss_d_real + loss_d_fake)\n",
    "        \n",
    "        # We use backpropagation\n",
    "        loss_d.backward(retain_graph=True)\n",
    "        \n",
    "        # We let ADAM do its shit\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        print(\"Image #: \" + str(i*4))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.swapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
