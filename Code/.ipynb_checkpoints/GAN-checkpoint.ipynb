{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import database as db\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANdiscriminator(nn.Module):\n",
    "    ''' This class implements a PatchGAN discriminator for a 100x100 image.\n",
    "        Small modification of the one used in:\n",
    "            - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks \n",
    "              Jun-Yan Zhu, 2017'''\n",
    "    \n",
    "    def __init__(self, n_image_channels = 3):\n",
    "        super(GANdiscriminator, self).__init__()\n",
    "                \n",
    "        def createLayer(n_filt_in, n_filt_out, ker_size, stride, norm = True, last = False):\n",
    "            ''' This function creates the differnt convolutional layers, all with same structure'''\n",
    "            layers = [nn.Conv2d(n_filt_in, n_filt_out, ker_size, stride=stride)]\n",
    "            if (norm):\n",
    "                layers.append(nn.InstanceNorm2d(n_filt_out)) # batch normalization\n",
    "            if (last):\n",
    "                layers.append(nn.Sigmoid()) # we output the probability\n",
    "            else:\n",
    "                layers.append(nn.LeakyReLU(negative_slope = 0.05, inplace=True)) # we use Leaky ReLU\n",
    "            return layers\n",
    "        \n",
    "        \n",
    "        ''' Input number of filters: Image channels\n",
    "            Intermediate number of filters: 64*h, with h being the depth of the layer\n",
    "            Output number of filters: 1 -> Decision of true or false\n",
    "            It takes patches of 61x61 pixels'''\n",
    "        layers = []\n",
    "        n_layers = 5\n",
    "        ker_size = 5\n",
    "        strides = [1, 1, 1, 2, 2]\n",
    "        n_filters = [n_image_channels, 64, 128, 256, 512, 1]\n",
    "        lasts = [False, False, False, False, True]\n",
    "        for i in range(n_layers): # For each layer\n",
    "            layers.extend(createLayer(n_filters[i], n_filters[i+1], ker_size, strides[i], last = lasts[i]))\n",
    "                \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.model(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual_block(nn.Module):\n",
    "    ''' This class implements the residual block of the RES net we will implement as the generator'''\n",
    "    def __init__(self, n_channels):\n",
    "        super(residual_block, self).__init__()\n",
    "        \n",
    "        layers = [ \n",
    "                  nn.ReflectionPad2d(1), # mirroring of 1 for the 3 kernel size convolution\n",
    "                  nn.Conv2d(n_channels, n_channels, 3), # the convolution :)\n",
    "                  nn.InstanceNorm2d(n_channels), # batch normalization\n",
    "                  nn.LeakyReLU(negative_slope=0.05, inplace=True), \n",
    "                  # We repeat the process\n",
    "                  nn.ReflectionPad2d(1), \n",
    "                  nn.Conv2d(n_channels, n_channels, 3),\n",
    "                  nn.InstanceNorm2d(n_channels)\n",
    "                 ]\n",
    "        \n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return image + self.conv_block(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANgenerator(nn.Module):\n",
    "    ''' This class implements a RES Net for generating the images\n",
    "        Small modification of the one defined in:\n",
    "            - Deep Residual Learning for Image Recognition\n",
    "              Kaiming He, 2015'''\n",
    "    \n",
    "    def __init__(self, n_image_channels = 3, n_res_blocks = 9):\n",
    "        super(GANgenerator, self).__init__()\n",
    "        \n",
    "        ''' High kernel convolution '''\n",
    "        n_channels_high = 64\n",
    "        layers = [ nn.ReflectionPad2d(3), # mirroring of 3 for the 7 kernel size convolution\n",
    "                    nn.Conv2d(n_image_channels, n_channels_high, 7), # 64 new channels of 7x7 convolution :)\n",
    "                    nn.InstanceNorm2d(n_channels_high),\n",
    "                    nn.LeakyReLU(negative_slope=0.05, inplace=True)\n",
    "                  ]\n",
    "        \n",
    "        ''' Variables for down and up sampling '''\n",
    "        n_layers = 2\n",
    "        ker_size = 3\n",
    "        strides = 2\n",
    "        paddings = 1\n",
    "        n_filters = [n_channels_high, n_channels_high*2, n_channels_high*4]\n",
    "        \n",
    "        ''' Downsampling steps '''\n",
    "        for i in range(n_layers): # for each layer\n",
    "            layers.extend([ nn.Conv2d(n_filters[i], n_filters[i+1], ker_size, \\\n",
    "                                    strides, padding=paddings),\n",
    "                            nn.InstanceNorm2d(n_filters[i+1]),\n",
    "                            nn.LeakyReLU(negative_slope=0.05, inplace=True)])\n",
    "        \n",
    "        ''' Residual blocks '''\n",
    "        for i in range(n_res_blocks):\n",
    "            layers.extend([residual_block(n_filters[-1])]) # the residual blocks are applied to the \n",
    "                                                           # last number of channels in the down sampling\n",
    "        \n",
    "        ''' Upsampling steps '''\n",
    "        for i in range(n_layers): # for each layer\n",
    "            layers.extend([ nn.ConvTranspose2d(n_filters[-(i+1)], n_filters[-(i+2)], ker_size, \\\n",
    "                                    strides, padding=paddings, output_padding=1),\n",
    "                            nn.InstanceNorm2d(n_filters[-(i+2)]),\n",
    "                            nn.LeakyReLU(negative_slope=0.05, inplace=True)])\n",
    "        ''' Output '''\n",
    "        layers.extend([ nn.ReflectionPad2d(3), # mirroring of 3 for the 7 kernel size convolution\n",
    "                        nn.Conv2d(n_channels_high, n_image_channels, 7), # 64 new channels of 7x7 convolution :)\n",
    "                        nn.Sigmoid() ])\n",
    "                \n",
    "        self.res_net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        return self.res_net(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "criterion_gan = nn.BCELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for using cuda or not\n",
    "cuda = True if torch.cuda.is_available() else False # for using the GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of patches (61x61) separated 25\n",
    "batch_size = 1\n",
    "img_x = 128\n",
    "img_y = 128\n",
    "separation = 25\n",
    "patch_x, patch_y = 26, 26\n",
    "patch = (batch_size, 1 , patch_x, patch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of the discriminator and generator\n",
    "D_A = GANdiscriminator()\n",
    "D_B = GANdiscriminator()\n",
    "G_AB = GANgenerator()\n",
    "G_BA = GANgenerator()\n",
    "if cuda:\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "    G_AB = G_AB.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    criterion_gan.cuda()\n",
    "    criterion_cycle.cuda()\n",
    "    criterion_identity.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.kaiming_normal(m.weight.data, a=0.05)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.kaiming_normal(m.weight.data, a=0.05)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "G_AB.apply(weights_init); # He initialization of the weights\n",
    "G_BA.apply(weights_init); # He initialization of the weights\n",
    "D_A.apply(weights_init); # He initialization of the weights\n",
    "D_B.apply(weights_init); # He initialization of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer for the generator\n",
    "lr = 1e-4\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.99\n",
    "gan_params = list(G_AB.parameters()) + list(G_BA.parameters())\n",
    "optimizer_G = torch.optim.Adam(gan_params, lr = lr, betas=(beta_1, beta_2))\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_update():\n",
    "    def __init__(self, n_epochs, epoch, start_decay_epoch):\n",
    "        assert ((n_epochs - start_decay_epoch) > 0), \"You can't decay after finishing\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epoch = epoch\n",
    "        self.start_decay_epoch = start_decay_epoch\n",
    "    \n",
    "    def decay(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.epoch - self.start_decay_epoch) \\\n",
    "            / (self.n_epochs - self.start_decay_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler for optimization\n",
    "n_epochs = 250\n",
    "epoch = 0\n",
    "start_decay_epoch = 100\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, \\\n",
    "    lr_lambda=lr_update(n_epochs, epoch, start_decay_epoch).decay)\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, \\\n",
    "    lr_lambda=lr_update(n_epochs, epoch, start_decay_epoch).decay)\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, \\\n",
    "    lr_lambda=lr_update(n_epochs, epoch, start_decay_epoch).decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure for using cuda tensors or just cpu tensors\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to store the images\n",
    "class imageBuffer():\n",
    "    def __init__(self, max_size = 50):\n",
    "        assert (max_size > 0), 'You need to be able to store something'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "    \n",
    "    def push_and_pop(self, data):\n",
    "        result = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            # If we can store data\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                result.append(element)\n",
    "            # Else change a random element of the data with probability 0.5\n",
    "            else:\n",
    "                if random.uniform(0,1) > 0.5:\n",
    "                    pos = random.randint(0, self.max_size-1)\n",
    "                    result.append(self.data[pos].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    result.append(element)\n",
    "        # Return the result as a torch variable\n",
    "        return Variable(torch.cat(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffers of previously generated samples \n",
    "fake_A_buffer = imageBuffer()\n",
    "fake_B_buffer = imageBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "fruits_file = 'Dataset/dataset_index.csv'\n",
    "textures_file = 'Dataset/textures_index.csv'\n",
    "textures = db.TexturesDataset(csv_file=textures_file)\n",
    "imgs_db = db.FruitsDataset(csv_file=fruits_file, cl_A='Peach', cl_B='Orange', \n",
    "                        transform = transforms.Compose([db.ChangeBackground(textures), db.myReshape()]))\n",
    "\n",
    "dataloader = DataLoader(imgs_db, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(imgs_db, batch_size=5,\n",
    "                        shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(num_batches):\n",
    "    ''' Saves a generated sample from the validation set'''\n",
    "    img_A = next(iter(val_dataloader_A))\n",
    "    img_B = next(iter(val_dataloader_B))\n",
    "    real_A = Variable(img_A.type(Tensor))\n",
    "    fake_B = G_AB(real_A)\n",
    "    real_B = Variable(img_B.type(Tensor))\n",
    "    fake_A = G_BA(real_B)\n",
    "    ## Gudardar-les després és cosa d'en Marcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = Variable(Tensor(np.ones(patch)), requires_grad=False)\n",
    "fake = Variable(Tensor(np.zeros(patch)), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################\n",
      "- EPOCH: 0\n",
      "##########################################################\n",
      "Image #: 0\n",
      "Image #: 1\n",
      "Image #: 2\n",
      "Image #: 3\n",
      "Image #: 4\n",
      "Image #: 5\n",
      "Image #: 6\n",
      "Image #: 7\n",
      "Image #: 8\n",
      "Image #: 9\n",
      "Image #: 10\n",
      "Image #: 11\n",
      "Image #: 12\n",
      "Image #: 13\n",
      "Image #: 14\n",
      "Image #: 15\n",
      "Image #: 16\n",
      "Image #: 17\n",
      "Image #: 18\n",
      "Image #: 19\n",
      "Image #: 20\n",
      "Image #: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8:\n",
      "Process Process-6:\n",
      "Process Process-7:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-341337495524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Get model input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mreal_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mreal_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training phase\n",
    "\n",
    "alpha_cycle = 10\n",
    "alpha_identy = 5\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"##########################################################\")\n",
    "    print(\"- EPOCH: \" + str(epoch))\n",
    "    print(\"##########################################################\")\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Get model input\n",
    "        real_A = Variable(batch[0].type(Tensor))\n",
    "        real_B = Variable(batch[1].type(Tensor))\n",
    "        \n",
    "        # ----------- #\n",
    "        #  Generator  #\n",
    "        # ----------- #\n",
    "        \n",
    "        # Set the optimizer\n",
    "        optimizer_G.zero_grad()    \n",
    "        \n",
    "        # Set the identity loss \n",
    "        loss_identity_A = criterion_identity(G_BA(real_A), real_A)\n",
    "        loss_identity_B = criterion_identity(G_AB(real_B), real_B)\n",
    "        loss_identity = 0.5*(loss_identity_A + loss_identity_B)\n",
    "        \n",
    "        # Generate two images\n",
    "        fake_B = G_AB(real_A)\n",
    "        fake_A = G_BA(real_B)\n",
    "        \n",
    "        # Set GAN loss\n",
    "        loss_gan_AB = criterion_gan(D_B(fake_B), valid) # the discriminator finds B real enough\n",
    "        loss_gan_BA = criterion_gan(D_A(fake_A), valid) # the discriminarot finds A real enough\n",
    "        loss_gan = 0.5*(loss_gan_AB + loss_gan_BA)\n",
    "        \n",
    "        # \"recover\" the two images\n",
    "        recovered_A = G_BA(fake_B)\n",
    "        recovered_B = G_AB(fake_A)\n",
    "        \n",
    "        # Set cycle loss\n",
    "        loss_cycle_A = criterion_cycle(recovered_A, real_A)\n",
    "        loss_cycle_B = criterion_cycle(recovered_B, real_B)\n",
    "        loss_cycle = 0.5*(loss_cycle_A + loss_cycle_B)\n",
    "        \n",
    "        # Total loss\n",
    "        loss_total = loss_gan + alpha_cycle * loss_cycle_A #+ alpha_id * loss_identity \n",
    "        \n",
    "        # Backpropagate the gradient of the loss\n",
    "        loss_total.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # -------------- #\n",
    "        #  Discriminator #\n",
    "        # -------------- #\n",
    "        \n",
    "        # Set the optimizer\n",
    "        optimizer_D_A.zero_grad()\n",
    "        optimizer_D_B.zero_grad()\n",
    "        \n",
    "        # Get the real loss\n",
    "        loss_real_A = criterion_gan(D_A(real_A), valid)\n",
    "        loss_real_B = criterion_gan(D_B(real_B), valid)\n",
    "        \n",
    "        # Fake loss (on previously generated samples)\n",
    "        fake_A_prev = fake_A_buffer.push_and_pop(fake_A)\n",
    "        fake_B_prev = fake_B_buffer.push_and_pop(fake_B)\n",
    "        loss_fake_A = criterion_gan(D_A(fake_A_prev.detach()), fake)\n",
    "        loss_fake_B = criterion_gan(D_B(fake_B_prev.detach()), fake)\n",
    "        \n",
    "        # Total losses\n",
    "        loss_D_A = 0.5*(loss_real_A + loss_fake_A)\n",
    "        loss_D_B = 0.5*(loss_real_B + loss_fake_B)\n",
    "        \n",
    "        # Backpropagate the gradient of the losses\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        # Final total loss for discriminator (only for plotting purposes)\n",
    "        loss_D = 0.5*(loss_D_A + loss_D_B)\n",
    "        \n",
    "        # Saving stuff and shit... (marcel ^.^)\n",
    "        \n",
    "        print(\"Image #: \" + str(i*batch_size))\n",
    "    \n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "    \n",
    "    # Saving models... (agin marcel ^.^)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create some noise\n",
    "noise = Tensor(np.random.normal(loc = 0.0, \\\n",
    "    scale= 0.9, size=batch.shape))\n",
    "\n",
    "noise = Tensor(np.zeros(batch.shape))\n",
    "\n",
    "# Generate some images from the noise\n",
    "generated = G(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = generated[1].data.cpu().numpy()\n",
    "v = np.transpose(v, (2,1,0))\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.imshow(v)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " loss_g = criterion(D(generated),valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_d_real = criterion(D(input_img),valid)\n",
    "loss_d_fake = criterion(D(generated),fake)\n",
    "loss_d = 0.5 * (loss_d_real + loss_d_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_d_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_d_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_img = Tensor(batch_size,3,img_x,img_y)\n",
    "valid = Variable(Tensor(np.ones(patch)), requires_grad=False)\n",
    "fake = Variable(Tensor(np.zeros(patch)), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 % 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
